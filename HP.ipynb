{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96d9afc-2dd4-4d2a-a627-c5ca9f000074",
   "metadata": {},
   "source": [
    "# HARRY POTTER TEXT MINING DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d73e0-0262-49c9-ad0d-363b97ca24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from string import punctuation\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # Sentiment analysis\n",
    "import pprint # to print dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a304ad0-ed0b-4eaa-918d-430b4d3d8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def text_analyzer(text, stemmer, stop_words):\n",
    "    text = re.sub(r\"http\\S+\", \" link \",text)\n",
    "    text = word_tokenize(text)\n",
    "    text = [token for token in text if token not in stop_words]\n",
    "    text = [stemmer.stem(token) for token in text]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a790f0c-d666-426f-9610-64311ec27ca8",
   "metadata": {},
   "source": [
    "read the single book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02156a-aa5a-428a-ba69-87b2399a82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hp(title_path):\n",
    "    with open(title_path, \"r\", encoding=\"utf8\") as book:\n",
    "        # Reads each line removing spaces before and after\n",
    "        lines = [line.strip() for line in book.readlines() if not (line.startswith(\"Page |\") or line.strip() == '')]\n",
    "    # Join the lines into a single string\n",
    "    text = '\\n'.join(lines)\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"CHPT\", \"\")\n",
    "\n",
    "    return text\n",
    "book1 = read_hp('Book1.txt')\n",
    "book2 = read_hp('Book2.txt')\n",
    "book3 = read_hp('Book3.txt')\n",
    "book4 = read_hp('Book4.txt')\n",
    "book5 = read_hp('Book5.txt')\n",
    "book6 = read_hp('Book6.txt')\n",
    "book7 = read_hp('Book7.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3dd69-fbe5-4a7c-b112-164573c9b4d3",
   "metadata": {},
   "source": [
    "I collect all the books together and transform to the lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c1fa1-5a66-448f-a281-54e1a5fcd321",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = book1 + book2 + book3 + book4 + book5 + book6 + book7\n",
    "books = books.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d4a42-d0d2-4968-ba72-a9f10b22c516",
   "metadata": {},
   "source": [
    "I start managing my text:\n",
    "1. create my own tokenizer:\n",
    "    1. consider all the words\n",
    "    2. consider common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0787eb1-caf8-4328-bbd7-3421de990ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        self.patterns = [r\"\\b\\w+'t\\b|\\b\\w+\\b|'\\w+\\b\"]\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for pattern in self.patterns:\n",
    "            regex = re.compile(pattern)\n",
    "            matches = regex.finditer(text)\n",
    "            for match in matches:\n",
    "                tokens.append(match.group())\n",
    "        return tokens\n",
    "\n",
    "# Example usage:\n",
    "custom_tokenizer = CustomTokenizer()\n",
    "text = \"Hello, World! This is a custom tokenizer. mr. how are you. and maybe's jr. st. or whatever three-quarters don't jarry's\"\n",
    "tokens = custom_tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72234f3c-c343-4edd-9110-22215fc4e06f",
   "metadata": {},
   "source": [
    "from this list I want to keep only the words without considering the punctuation and the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbe1a7-a4c6-45c9-ae5e-66efc6f5e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with all the tokens of all the books\n",
    "hp_tokens = custom_tokenizer.tokenize(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d031b-36f5-4130-8bc2-9b146ab590b1",
   "metadata": {},
   "source": [
    "now that i have the tokens from the books, i'm going to delete the stop words (taken from a common dict and defined by us using some frequences statics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690fdd4-8af9-44f5-95c2-2a3d2b451cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stop_words.txt\", 'r') as file:\n",
    "    hp_stop_w = [words.strip() for words in file.readlines()]\n",
    "\n",
    "hp_tokens_sw = [i for i in hp_tokens if i not in hp_stop_w]\n",
    "hp_tokens_sw = [word for word in hp_tokens_sw if not word.startswith(\"'\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b208ae-25ef-4481-a6e7-4b41584e5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_counter = Counter(hp_tokens_sw)\n",
    "N = 45\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "\n",
    "plt.title(\"{} Most frequent words in the Harry Potter series\".format(N))\n",
    "plt.bar(*zip(*hp_counter.most_common(N)), color=\"gold\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3c937-0789-4c98-88e9-07188397f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(hp_tokens_sw))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfe4de-84d8-4cb1-be31-0e8e27bef9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "MASK = np.array(Image.open(\"Sorting_Hat.png\"))\n",
    "MAX_WORDS = 200\n",
    "MAX_FONT_SIZE = 500\n",
    "RELATIVE_SCALING = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086a2ff-5c7b-46ce-89fd-513c5a4eb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = WordCloud(\n",
    "    width=500, \n",
    "    height=300,\n",
    "    mask = MASK,\n",
    "    max_words = MAX_WORDS, \n",
    "    background_color = \"white\",\n",
    "    max_font_size = MAX_FONT_SIZE,\n",
    "    relative_scaling = RELATIVE_SCALING,\n",
    ").generate_from_frequencies(hp_counter)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(hp, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2155f-19a8-4b6f-9502-a32e943436cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_stem = [stemmer.stem(token) for token in hp_tokens_sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dec9d1-f778-4f5c-a2a6-98173ce8bccb",
   "metadata": {},
   "source": [
    "## ORA LO FACCIAMO PER LIBRO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f607a1-5295-4804-9f28-885219e89708",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_books = [book1, book2, book3, book4, book5, book6, book7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ef664-9116-4deb-878d-b03328f62d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(text, custom_tokenizer, stop_words): \n",
    "    text = text.lower()\n",
    "    text = custom_tokenizer.tokenize(text)\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    text = [w for w in text if not w.startswith(\"'\")]\n",
    "    text = [w for w in text if len(w)>1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd01d13-ec40-4a0c-8df7-edb5f2925046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you are in a loop\n",
    "books_hp_token = {}  # Dictionary to store the lists\n",
    "\n",
    "for i in range(len(hp_books)):  # Replace 5 with the desired number of iterations\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    # book_token = []  # Create a new list for each iteration\n",
    "    books_hp_token[book_name] = preproc(hp_books[i], custom_tokenizer, stop_words = hp_stop_w)\n",
    "\n",
    "books_hp_stem = {}\n",
    "for i in range(len(hp_books)):  # Replace 5 with the desired number of iterations\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    # book_token = []  # Create a new list for each iteration\n",
    "    books_hp_stem[book_name] = [stemmer.stem(token) for token in books_hp_token[book_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ec3b8-c672-4e52-9a60-cfc0bf83cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_hp_token[\"book_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00872ed1-1574-4380-b763-cf4245d0a1eb",
   "metadata": {},
   "source": [
    "## PLATONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875728c1-1cde-448b-a04c-435d9b57c66b",
   "metadata": {},
   "source": [
    "### SPIEGONE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca0f86-c8b7-4962-9986-257f8ff43084",
   "metadata": {},
   "source": [
    "Inizialmente partiamo prendendo le 400 parole pi√π frequenti all'interno del primo e del settimo libro (che saranno gli estremi del nostro ordinamento di libri) e di queste parole, terremo quelle che sono presenti anche negli altri 5 libri eliminando le parole che ai fini dell'analisi non ci sembrano rilevanti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90df9c-55d4-4c46-9298-72bab59c1c76",
   "metadata": {},
   "source": [
    "Dentro `books_hp_stem` abbiamo gli stemmi per ciascun libro, ora ci creiamo un dizionario che conti le parole del primo e del settimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fe20a-eff7-42f2-88f1-059608942a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Counter(books_hp_stem['book_1'])\n",
    "book11 = dict(sorted(x.items(),key=lambda item: item[1], reverse=True))\n",
    "y = Counter(books_hp_stem['book_7'])\n",
    "book77 = dict(sorted(y.items(),key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a698a-77c3-444b-9ee1-e1a28321090d",
   "metadata": {},
   "source": [
    "Ora quindi ci teniamo le prime 400 parole dal primo e dal settimo libro che sono quelli che mi definiranno l'ordinamento dei libri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee41620-65f6-4872-9762-e71a7301670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the most common stems that are in book 1 and book 7\n",
    "book11 = dict(list(book11.items())[:400])\n",
    "book77 = dict(list(book77.items())[:400])\n",
    "most_comm_17 = list(set(book11.keys()) & set(book77.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3d7cf-003c-4a61-a63d-24d515e84d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_comm_tot = most_comm_17.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f74dd-362d-49b1-a664-1c64118530f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the most common stems that are also in the other books\n",
    "for i in range(len(books_hp_stem)):  # Replace 5 with the desired number of iterations\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    most_comm_tot = [i for i in most_comm_tot if i in books_hp_stem[book_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480fe12-d12a-4820-a23f-74f04fe9c6d6",
   "metadata": {},
   "source": [
    "We consider two initial population $W_1$ = *Harry Potter and the Phylosopher stone* and $W_7$ = *Harry Potter and the deathly hallows*. Let $n_{i1}$ the frequence of the *i*-th stem for *i* = 1, $\\dots$, $N_{stems}$ in book 1 and let $N_1 = \\sum_{i}n_{i1}$ and $N_7$ the corrispondent sum for $W_7$. We indicate with $\\theta_{1i}$, for $i = 1, \\dots, p$ the probability that the i-th word is in $W_1$ and $\\theta_{7i}$ for $i = 1, \\dots, p$ the same quantity for $W_7$. The probability to observe a sample from $W_1$ and $W_7$ follows a multinomial distribution. Using the log likelihood ratio test to compare the two samples we get the expression:\n",
    "\n",
    "$$ \\sum_{i=1}^{N_{stem}} n_i\\log\\frac{\\theta_{7i}}{\\theta_{1i}}$$\n",
    "\n",
    "We can see that every word has a sort of score, like:\n",
    "\n",
    "$$ s_i = \\log{\\frac{\\theta_{7i}}{\\theta_{1i}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480b581-a498-4ad3-845b-c8169ec30961",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict = {key: value for key, value in Counter(books_hp_stem['book_1']).items() if key in most_comm_tot}\n",
    "xx = sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True)\n",
    "df_book = pd.DataFrame(xx, columns = ['Stems', 'Count_1'])\n",
    "df_book['Freq_1'] = df_book['Count_1']/df_book['Count_1'].sum()\n",
    "df_book.head()\n",
    "\n",
    "filtered_dict = {key: value for key, value in Counter(books_hp_stem['book_7']).items() if key in most_comm_tot}\n",
    "xx = dict(sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True))\n",
    "df_book['Count_7'] = df_book['Stems'].map(xx)\n",
    "df_book['Freq_7'] = df_book['Stems'].map(xx)/df_book['Stems'].map(xx).sum()\n",
    "df_book.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e4989-a8bd-4075-b54c-b94a1158ed7f",
   "metadata": {},
   "source": [
    "Now the idea is to extend the analysis to the other books, which represent new populations to be classified, and use the total score of each book as a discriminant function. Thus let $W_k$ with $k = 2,3,4,5,6$ be the populations that represent the books from 2 to 6. For these populations, the same results apply as for $W_1$ to $W_7$. So for each $W_k$ with $k = 2,3,4,5,6,7$ we can calculate a measure that represents the comprehensive score assigned to each $W_k$. In particular for each $W_k$ where $N_k = \\sum_i n_{ik}$ one can calculate the average score\n",
    "\n",
    "$$\n",
    "s_k = \\frac{1}{N_k} \\sum_{i=1}^{N_{stem}} n_{i} \\log \\frac{\\theta_{1i}}{\\theta_{7i}} = \\sum_{i=1}^{N_{stem}} n_{i}s_i \\quad i = 1, ..., N_{stem}.\n",
    "$$\n",
    "\n",
    "To calculate the average scores we need to estimate the parameter vectors $\\theta_1$ and $\\theta_7$. Since both $N_1$ and $N_7$ are high, the probabilities $\\hat{\\theta}_{i1}$ and $\\hat{\\theta}_{i7}$ can be estimated with the corresponding observed frequencies. So we find for all five books to be classified an associated score that represents the positioning of that book. This measure can be interpreted in relative terms to understand which are the furthest books and which are the closest. To make inferences and evaluate the significance of the results obtained, we can also define the variance of $s_k$, for which an unbiased estimate is\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{V}(\\bar{s}_k) = \\frac{1}{N_k(N_k - 1)} \\left( \\sum_{i=1}^{N_{stem}} n_i s_i^2 - \\frac{1}{N_k} \\left( \\sum_{i=1}^{N_{stem}} n_i s_i \\right)^2 \\right).\n",
    "$$\n",
    "\n",
    "Given that $N_k$ is large, $s_k - s_k'$ will be approximately normally distributed with variance equal to the sum of the corresponding variances; therefore, to test the significance, we can use the usual $t$-test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442e63b-61c2-4876-9fb1-e18333c99374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function create the counts and the freq for every book\n",
    "def hp_count(idx):\n",
    "    #create the dict with the frequencies for the i-th book\n",
    "    xx = {key: value for key, value in Counter(books_hp_stem[idx]).items() if key in most_comm_tot}\n",
    "    count = df_book['Stems'].map(xx)\n",
    "    freq  = df_book['Stems'].map(xx)/df_book['Stems'].map(xx).sum()\n",
    "    df = pd.DataFrame\n",
    "    return count, freq\n",
    "\n",
    "# with this loop we add a column for each of the book with their own freq and counts\n",
    "for i in range(2, 7):\n",
    "    book_idx = f\"book_{i}\"\n",
    "    col_count, col_freq = hp_count(book_idx)\n",
    "    df_book[f\"Count_{i}\"] = np.array(col_count)\n",
    "    df_book[f\"Freq_{i}\"]= np.array(col_freq)\n",
    "\n",
    "df_book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa62d17-e66d-44bd-8f73-5b111e87b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_scores(df, idx):\n",
    "    N = df[f\"Count_{idx}\"].sum()\n",
    "    n = df[f\"Count_{idx}\"]\n",
    "    theta1 = df['Freq_1']\n",
    "    theta7 = df['Freq_7']\n",
    "    return round(1/N * sum(n*np.log(theta1/theta7)), 4)\n",
    "\n",
    "scores = {}\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"] = {'score': hp_scores(df_book, i)}\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key=lambda item: item[1]['score'], reverse = True)\n",
    "scores = {k: v for k, v in sorted_scores}\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fd7e2-3c73-44b3-b50f-897f704f36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_var(df, idx):\n",
    "    N = df[f\"Count_{idx}\"].sum()\n",
    "    n = df[f\"Count_{idx}\"]\n",
    "    theta1 = df['Freq_1']\n",
    "    theta7 = df['Freq_7']\n",
    "    return round(1/(N * (N - 1))*(sum(n * np.log(theta1/theta7)**2) - 1/N * (sum(n * np.log(theta1/theta7))**2)), 6)\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"]['Variance'] = hp_var(df_book, i)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8e20e-b90d-4233-a2e5-126b6573bf41",
   "metadata": {},
   "source": [
    "### t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046271a-6933-4000-8676-41070fb8b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(dict):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96884776-e99e-44e9-b569-53eab01ebe7c",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf70b56-5acc-4356-9f1c-3186b07c17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246047b-a1c3-478f-bbff-f106e03980a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_names = list()\n",
    "for i in range(len(hp_books)): \n",
    "    book_names.append(f\"book_{i+1}\")\n",
    "print(book_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8001c3-004d-4c6c-9f8b-a648d58acb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "for b in book_names:\n",
    "    new_b = \" \".join(books_hp_token[b])\n",
    "    scores[b] = analyzer.polarity_scores(new_b)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632810dd-8487-4805-a16d-487a2da94cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "pp.pprint(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85374894-3216-44de-9ed6-6cedc35181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1][\"neg\"]))\n",
    "for key in sorted_scores:\n",
    "    print(key + \": \")\n",
    "    print(sorted_scores[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d4a53ad-d215-4028-88e2-bcfccf01e0e5",
   "metadata": {},
   "source": [
    "As we can see the books are perfectly ordered for the negative score, which means that in each book, even though there isn't a worse feeling with year after year, we can find a growing presence of you know who and of the death eaters, sometimes compensated by positive feelings and experiences of the main characters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
