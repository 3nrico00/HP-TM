{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96d9afc-2dd4-4d2a-a627-c5ca9f000074",
   "metadata": {},
   "source": [
    "# HARRY POTTER TEXT MINING DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d73e0-0262-49c9-ad0d-363b97ca24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# # from func import prova\n",
    "# # from func.prova import text_analyzer, stemmer, stop_words\n",
    "# import tqdm\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from collections import Counter\n",
    "# from wordcloud import WordCloud\n",
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# # from func import prova\n",
    "# from string import punctuation\n",
    "# import re\n",
    "# from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a256732-1c14-4251-ac67-d2b0c98ab86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from string import punctuation\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # Sentiment analysis\n",
    "import pprint # to print dictionaries\n",
    "from nrclex import NRCLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d9597-8ab6-45da-a0b8-d07c63562c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d831f-9da9-4920-ae78-6a6b6207db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analyzer(text, stemmer, stop_words):\n",
    "    text = re.sub(r\"http\\S+\", \" link \",text)\n",
    "    text = word_tokenize(text)\n",
    "    text = [token for token in text if token not in stop_words]\n",
    "    text = [stemmer.stem(token) for token in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a790f0c-d666-426f-9610-64311ec27ca8",
   "metadata": {},
   "source": [
    "read the single book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02156a-aa5a-428a-ba69-87b2399a82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hp(title_path):\n",
    "    with open(title_path, \"r\", encoding=\"utf8\") as book:\n",
    "        lines = [line.strip() for line in book.readlines() if not (line.startswith(\"Page |\") or line.strip() == '')]\n",
    "    # Join the lines into a single string\n",
    "    text = '\\n'.join(lines)\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"CHPT\", \"\")\n",
    "    \n",
    "    return text\n",
    "book1 = read_hp('Book1.txt')\n",
    "book2 = read_hp('Book2.txt')\n",
    "book3 = read_hp('Book3.txt')\n",
    "book4 = read_hp('Book4.txt')\n",
    "book5 = read_hp('Book5.txt')\n",
    "book6 = read_hp('Book6.txt')\n",
    "book7 = read_hp('Book7.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3dd69-fbe5-4a7c-b112-164573c9b4d3",
   "metadata": {},
   "source": [
    "I collect all the books together and transform to the lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c1fa1-5a66-448f-a281-54e1a5fcd321",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = book1 + book2 + book3 + book4 + book5 + book6 + book7\n",
    "books = books.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d4a42-d0d2-4968-ba72-a9f10b22c516",
   "metadata": {},
   "source": [
    "I start managing my text:\n",
    "1. create my own tokenizer:\n",
    "    1. consider all the words\n",
    "    2. consider common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0787eb1-caf8-4328-bbd7-3421de990ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        self.patterns = [\n",
    "            #(r'\\w+', 'WORD'),         # Matches words\n",
    "            #(r'\\d+', 'NUMBER'),        # Matches numbers\n",
    "            #(r'[.,;!?]', 'PUNCTUATION'),  # Matches common punctuation\n",
    "            #(r\"\\b(?:\\w+'\\w*|\\w+?n't)\\b\", 'abbreviation')\n",
    "            (r\"\\b\\w+'t\\b|\\b\\w+\\b|'\\w+\\b\", \"WORD\")\n",
    "            # (r'\\b[A-Za-z]+\\.(?![a-z])', 'WORD')  # Matches sequences of capitalized words (potential sentences)\n",
    "            # (r'\\b(?:[A-Za-z]+\\.?\\'?[A-Za-z]*|\\w+)\\b', 'WORD'),\n",
    "            #(r\"\\b(?:\\w+'\\w*|(?<!\\w)'(?:t|re|s|m|ll|ve)\\b|\\w+)\\b\", 'WORD')\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for pattern, token_type in self.patterns:\n",
    "            regex = re.compile(pattern)\n",
    "            matches = regex.finditer(text)\n",
    "            for match in matches:\n",
    "                tokens.append((match.group(), token_type))\n",
    "        return tokens\n",
    "\n",
    "# Example usage:\n",
    "custom_tokenizer = CustomTokenizer()\n",
    "text = \"Hello, World! This is a custom tokenizer. mr. how are you. and maybe's jr. st. or whatever three-quarters don't jarry's\"\n",
    "tokens = custom_tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72234f3c-c343-4edd-9110-22215fc4e06f",
   "metadata": {},
   "source": [
    "from this list I want to keep only the words without considering the punctuation and the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbe1a7-a4c6-45c9-ae5e-66efc6f5e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_tokens = custom_tokenizer.tokenize(books)\n",
    "hp_tokens = [i[0] for i in hp_tokens if i[1] == 'WORD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d031b-36f5-4130-8bc2-9b146ab590b1",
   "metadata": {},
   "source": [
    "now that i have the tokens from the books, i'm going to delete the stop words (taken from a common dict and defined by us using some frequences statics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690fdd4-8af9-44f5-95c2-2a3d2b451cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp_tokens_sw = [i for i in hp_tokens if i not in ENGLISH_STOP_WORDS]\n",
    "# stop_words = stopwords.words('english')\n",
    "# hp_stop_w = list(set(list(ENGLISH_STOP_WORDS) + stop_words))\n",
    "# print(hp_stop_w)\n",
    "with open(\"stop_words.txt\", 'r') as file:\n",
    "    hp_stop_w = [words.strip() for words in file.readlines() if not (words.startswith(\"Page |\") or words.strip() == '')]\n",
    "\n",
    "hp_tokens_sw = [i for i in hp_tokens if i not in hp_stop_w]\n",
    "hp_tokens_sw = [word for word in hp_tokens_sw if not word.startswith(\"'\")]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5eec0e3-38fc-44d0-b14c-e527380f2c1e",
   "metadata": {},
   "source": [
    "#hp_tokens_test = [i for i in hp_tokens if i not in stop_words]\n",
    "#from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "#hp_tokens_sw = [i for i in hp_tokens if i not in ENGLISH_STOP_WORDS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b208ae-25ef-4481-a6e7-4b41584e5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_counter = Counter(hp_tokens_sw)\n",
    "# hp_counter2 = Counter(hp_tokens_test)\n",
    "N = 45\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# plt.subplot(121)\n",
    "plt.title(\"{} Most frequent words in the Harry Potter series\".format(N))\n",
    "plt.bar(*zip(*hp_counter.most_common(N)), color=\"gold\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3c937-0789-4c98-88e9-07188397f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(hp_tokens_sw))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfe4de-84d8-4cb1-be31-0e8e27bef9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "MASK = np.array(Image.open(\"Sorting_Hat.png\"))\n",
    "MAX_WORDS = 200\n",
    "MAX_FONT_SIZE = 500\n",
    "RELATIVE_SCALING = 0.7\n",
    "\n",
    "hp = WordCloud(\n",
    "    width=500, \n",
    "    height=300,\n",
    "    mask = MASK,\n",
    "    max_words = MAX_WORDS, \n",
    "    background_color = \"white\",\n",
    "    max_font_size = MAX_FONT_SIZE,\n",
    "    relative_scaling = RELATIVE_SCALING,\n",
    ").generate_from_frequencies(hp_counter)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(hp, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2155f-19a8-4b6f-9502-a32e943436cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_stem = [stemmer.stem(token) for token in hp_tokens_sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dec9d1-f778-4f5c-a2a6-98173ce8bccb",
   "metadata": {},
   "source": [
    "## ORA LO FACCIAMO PER LIBRO "
   ]
  },
  {
   "cell_type": "raw",
   "id": "959dd2d7-579c-4ab8-ae9c-69976d03b669",
   "metadata": {},
   "source": [
    "hp_tokens = custom_tokenizer.tokenize(books)\n",
    "hp_tokens = [i[0] for i in hp_tokens if i[1] == 'WORD']\n",
    "hp_tokens_sw = [i for i in hp_tokens if i not in ENGLISH_STOP_WORDS]\n",
    "stop_words = stopwords.words('english')\n",
    "hp_stop_w = list(set(list(ENGLISH_STOP_WORDS) + stop_words))\n",
    "# print(hp_stop_w)\n",
    "hp_tokens_sw = [i for i in hp_tokens if i not in hp_stop_w]\n",
    "hp_tokens_sw = [word for word in hp_tokens_sw if not word.startswith(\"'\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f607a1-5295-4804-9f28-885219e89708",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_books = [book1, book2, book3, book4, book5, book6, book7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ef664-9116-4deb-878d-b03328f62d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(text, custom_tokenizer, stop_words): \n",
    "    text = text.lower()\n",
    "    text = custom_tokenizer.tokenize(text)\n",
    "    text = [i[0] for i in text if i[1] == 'WORD']\n",
    "    text = [i for i in text if i not in stop_words]\n",
    "    text = [word for word in text if not word.startswith(\"'\")]\n",
    "    text = [i for i in text if len(i)>1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd01d13-ec40-4a0c-8df7-edb5f2925046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you are in a loop\n",
    "books_hp_token = {}  # Dictionary to store the lists\n",
    "\n",
    "for i in range(len(hp_books)):\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    books_hp_token[book_name] = preproc(hp_books[i], custom_tokenizer, stop_words = hp_stop_w)\n",
    "\n",
    "books_hp_stem = {}\n",
    "for i in range(len(hp_books)):\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    books_hp_stem[book_name] = [stemmer.stem(token) for token in books_hp_token[book_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ec3b8-c672-4e52-9a60-cfc0bf83cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_hp_stem[\"book_1\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00872ed1-1574-4380-b763-cf4245d0a1eb",
   "metadata": {},
   "source": [
    "## PLATONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875728c1-1cde-448b-a04c-435d9b57c66b",
   "metadata": {},
   "source": [
    "### SPIEGONE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca0f86-c8b7-4962-9986-257f8ff43084",
   "metadata": {},
   "source": [
    "Inizialmente partiamo prendendo le 400 parole più frequenti all'interno del primo e del settimo libro (che saranno gli estremi del nostro ordinamento di libri) e di queste parole, terremo quelle che sono presenti anche negli altri 5 libri eliminando le parole che ai fini dell'analisi non ci sembrano rilevanti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90df9c-55d4-4c46-9298-72bab59c1c76",
   "metadata": {},
   "source": [
    "Dentro `books_hp_stem` abbiamo gli stemmi per ciascun libro, ora ci creiamo un dizionario che conti le parole del primo e del settimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fe20a-eff7-42f2-88f1-059608942a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Counter(books_hp_stem['book_1'])\n",
    "book11 = dict(sorted(x.items(),key=lambda item: item[1], reverse=True))\n",
    "y = Counter(books_hp_stem['book_7'])\n",
    "book77 = dict(sorted(y.items(),key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a698a-77c3-444b-9ee1-e1a28321090d",
   "metadata": {},
   "source": [
    "Ora quindi ci teniamo le prime 400 parole dal primo e dal settimo libro che sono quelli che mi definiranno l'ordinamento dei libri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee41620-65f6-4872-9762-e71a7301670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the most common stems that are in book 1 and book 7\n",
    "book11 = dict(list(book11.items())[:400])\n",
    "book77 = dict(list(book77.items())[:400])\n",
    "most_comm_17 = list(set(book11.keys()) & set(book77.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3d7cf-003c-4a61-a63d-24d515e84d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_comm_tot = most_comm_17.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f74dd-362d-49b1-a664-1c64118530f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the most common stems that are also in the other books\n",
    "for i in range(len(books_hp_stem)):  # Replace 5 with the desired number of iterations\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    most_comm_tot = [i for i in most_comm_tot if i in books_hp_stem[book_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480fe12-d12a-4820-a23f-74f04fe9c6d6",
   "metadata": {},
   "source": [
    "We consider two initial population $W_1$ = *Harry Potter and the Phylosopher stone* and $W_7$ = *Harry Potter and the deathly hallows*. Let $n_{i1}$ the frequence of the *i*-th stem for *i* = 1, $\\dots$, $N_{stems}$ in book 1 and let $N_1 = \\sum_{i}n_{i1}$ and $N_7$ the corrispondent sum for $W_7$. We indicate with $\\theta_{1i}$, for $i = 1, \\dots, p$ the probability that the i-th word is in $W_1$ and $\\theta_{7i}$ for $i = 1, \\dots, p$ the same quantity for $W_7$. The probability to observe a sample from $W_1$ and $W_7$ follows a multinomial distribution. Using the log likelihood ratio test to compare the two samples we get the expression:\n",
    "\n",
    "$$ \\sum_{i=1}^{N_{stem}} n_i\\log\\frac{\\theta_{7i}}{\\theta_{1i}}$$\n",
    "\n",
    "We can see that every word has a sort of score, like:\n",
    "\n",
    "$$ s_i = \\log{\\frac{\\theta_{7i}}{\\theta_{1i}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480b581-a498-4ad3-845b-c8169ec30961",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict = {key: value for key, value in Counter(books_hp_stem['book_1']).items() if key in most_comm_tot}\n",
    "xx = sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True)\n",
    "df_book = pd.DataFrame(xx, columns = ['Stems', 'Count_1'])\n",
    "df_book['Freq_1'] = df_book['Count_1']/df_book['Count_1'].sum()\n",
    "df_book.head()\n",
    "\n",
    "filtered_dict = {key: value for key, value in Counter(books_hp_stem['book_7']).items() if key in most_comm_tot}\n",
    "xx = dict(sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True))\n",
    "df_book['Count_7'] = df_book['Stems'].map(xx)\n",
    "df_book['Freq_7'] = df_book['Stems'].map(xx)/df_book['Stems'].map(xx).sum()\n",
    "df_book.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e4989-a8bd-4075-b54c-b94a1158ed7f",
   "metadata": {},
   "source": [
    "Now the idea is to extend the analysis to the other books, which represent new populations to be classified, and use the total score of each book as a discriminant function. Thus let $W_k$ with $k = 2,3,4,5,6$ be the populations that represent the books from 2 to 6. For these populations, the same results apply as for $W_1$ to $W_7$. So for each $W_k$ with $k = 2,3,4,5,6,7$ we can calculate a measure that represents the comprehensive score assigned to each $W_k$. In particular for each $W_k$ where $N_k = \\sum_i n_{ik}$ one can calculate the average score\n",
    "\n",
    "$$\n",
    "\\bar{s}_k = \\frac{1}{N_k} \\sum_{i=1}^{N_{stem}} n_{i} \\log \\frac{\\theta_{1i}}{\\theta_{7i}} = \\sum_{i=1}^{N_{stem}} n_{i}s_i \\quad i = 1, ..., N_{stem}.\n",
    "$$\n",
    "\n",
    "To calculate the average scores we need to estimate the parameter vectors $\\theta_1$ and $\\theta_7$. Since both $N_1$ and $N_7$ are high, the probabilities $\\hat{\\theta}_{i1}$ and $\\hat{\\theta}_{i7}$ can be estimated with the corresponding observed frequencies. So we find for all five books to be classified an associated score that represents the positioning of that book. This measure can be interpreted in relative terms to understand which are the furthest books and which are the closest. To make inferences and evaluate the significance of the results obtained, we can also define the variance of $s_k$, for which an unbiased estimate is\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{V}(\\bar{s}_k) = \\frac{1}{N_k(N_k - 1)} \\left( \\sum_{i=1}^{N_{stem}} n_i s_i^2 - \\frac{1}{N_k} \\left( \\sum_{i=1}^{N_{stem}} n_i s_i \\right)^2 \\right).\n",
    "$$\n",
    "\n",
    "Given that $N_k$ is large, $s_k - s_k'$ will be approximately normally distributed with variance equal to the sum of the corresponding variances; therefore, to test the significance, we can use the usual $t$-test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442e63b-61c2-4876-9fb1-e18333c99374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function create the counts and the freq for every book\n",
    "def hp_count(idx):\n",
    "    #create the dict with the frequencies for the i-th book\n",
    "    xx = {key: value for key, value in Counter(books_hp_stem[idx]).items() if key in most_comm_tot}\n",
    "    count = df_book['Stems'].map(xx)\n",
    "    freq  = df_book['Stems'].map(xx)/df_book['Stems'].map(xx).sum()\n",
    "    df = pd.DataFrame\n",
    "    return count, freq\n",
    "\n",
    "# with this loop we add a column for each of the book with their own freq and counts\n",
    "for i in range(2, 7):\n",
    "    book_idx = f\"book_{i}\"\n",
    "    col_count, col_freq = hp_count(book_idx)\n",
    "    df_book[f\"Count_{i}\"] = np.array(col_count)\n",
    "    df_book[f\"Freq_{i}\"]= np.array(col_freq)\n",
    "\n",
    "df_book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa62d17-e66d-44bd-8f73-5b111e87b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_scores(df, idx):\n",
    "    N = df[f\"Count_{idx}\"].sum()\n",
    "    n = df[f\"Count_{idx}\"]\n",
    "    theta1 = df['Freq_1']\n",
    "    theta7 = df['Freq_7']\n",
    "    return round(1/N * sum(n*np.log(theta1/theta7)), 4)\n",
    "\n",
    "scores = {}\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"] = {'score': hp_scores(df_book, i)}\n",
    "#aggiunta\n",
    "    \n",
    "\n",
    "sorted_scores = sorted(scores.items(), key=lambda item: item[1]['score'], reverse = True)\n",
    "scores = {k: v for k, v in sorted_scores}\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fd7e2-3c73-44b3-b50f-897f704f36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_var(df, idx):\n",
    "    N = df[f\"Count_{idx}\"].sum()\n",
    "    n = df[f\"Count_{idx}\"]\n",
    "    theta1 = df['Freq_1']\n",
    "    theta7 = df['Freq_7']\n",
    "    return round(1/(N * (N - 1))*(sum(n * np.log(theta1/theta7)**2) - 1/N * (sum(n * np.log(theta1/theta7))**2)), 6)\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"]['Variance'] = hp_var(df_book, i)\n",
    "    scores[f\"book_{i}\"]['N'] = sum(df_book[f\"Count_{i}\"])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8e20e-b90d-4233-a2e5-126b6573bf41",
   "metadata": {},
   "source": [
    "### t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c252b-cbe7-48f7-836d-81f58b9ac592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def t_test(dict_1, dict_2):\n",
    "    numerator = dict_1[\"score\"] - dict_2[\"score\"]\n",
    "    denominator = np.sqrt(dict_1[\"Variance\"] + dict_2[\"Variance\"])\n",
    "\n",
    "    gdl = dict_1[\"N\"]+dict_2[\"N\"]-2\n",
    "    t_statistic = numerator / denominator\n",
    "    p_value = 2 * (1 - stats.t.cdf(np.abs(t_statistic), df=gdl))\n",
    "    return t_statistic, p_value\n",
    "\n",
    "t_statistic, p_value = t_test(scores[\"book_2\"], scores[\"book_6\"])\n",
    "\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800b58d-bcd0-4954-92a2-1a0b6ded889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest = {}\n",
    "for i in range(2,7):\n",
    "    for j in range(i+1,7):\n",
    "        ttest[f\"test_{i}_{j}\"] = {'t-stat': round(t_test(scores[f\"book_{i}\"], scores[f\"book_{j}\"])[0],3), \n",
    "                              'pval': round(t_test(scores[f\"book_{i}\"], scores[f\"book_{j}\"])[1], 3)}\n",
    "ttest\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9096cf-3c45-4cd3-a580-9304bead610c",
   "metadata": {},
   "source": [
    "Abbiamo detto che ci sta che i libri siano ordinati un po' a cazzum, infatti questo ordinamento è stato fatto utilizzando parole senza nessun criterio. Probabilmente l’insieme di parole scelto e molto generico e, considerando anche la vastit`a dei testi analizzati, vengono colti specifici costrutti lessicali, che non sono pero' legati alla trama e allo stile dei vari libri, ma sono il risultato dell’elevata dimensione dei libri.\n",
    "\n",
    "Inoltre il numero di parole scelto per l’analisi potrebbe essere troppo esiguo in relazione alla dimensione del problema.\n",
    "Abbiamo quindi replicato l’analisi dando una direzione piu specifica al problema, cioè assumendo l’ipotesi di un’evoluzione dello stile narrativo dell’autrice che ci ha portato a svolgere questo progetto. Partendo da diversi elenchi di parole categorizzate da dizionari ontologici come indicative di sentimenti negativi (come paura, tristezza, violenza) abbiamo selezionato manualmente all’interno di W1 e W7 le piu` frequenti, anche in relazione al contesto della saga. Dopo aver tenuto solamente le parole presenti in tutti e sette i libri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71242910-aef3-4ca4-b058-ca6828c74561",
   "metadata": {},
   "source": [
    "## SIMILARITY "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd785d-65a8-4d19-a5b0-81463cd862ca",
   "metadata": {},
   "source": [
    "We start from the stemmed words matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb9539-3de9-4e04-b5a8-087dda221187",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\" \".join(words) for words in books_hp_stem.values()]\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "count_array = count_matrix.toarray()\n",
    "\n",
    "freq_matrix = np.divide(count_array, np.sum(count_array, axis=1, keepdims=True))\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196547e3-8cd4-4c3f-83ef-7c887af6dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\" \".join(words) for words in books_hp_stem.values()]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(documents)\n",
    "count_array = count_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b47b4-35a2-47da-9a69-5b8cf2b8dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b99588-f604-4c6e-8477-dfdc32de1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(count_array, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0ab9e-c5a7-4aa0-9548-6563b10abc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(books_hp_stem['book_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80429598-19f4-4aeb-b6a2-c34a43612b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def cosine1(c,m): #c=centroidi m=intera\n",
    "    centroidi={el:[] for el in range(m.shape[0])}\n",
    "    for i in tqdm.tqdm(range(m.shape[0])):\n",
    "        for j in range(c.shape[0]):\n",
    "            centroidi[i].append(np.dot(m[i],c[j])/(norm(m[i])*norm(c[j])))\n",
    "    return(centroidi)\n",
    "\n",
    "def cosine(c,m): #c=centroidi m=intera\n",
    "    centroidi={el:[] for el in range(m.shape[0])}\n",
    "    for i in range(m.shape[0]):\n",
    "        for j in range(c.shape[0]):\n",
    "            centroidi[i].append(np.dot(m[i],c[j])/(norm(m[i])*norm(c[j])))\n",
    "    return(centroidi)\n",
    "\n",
    "def centroidi(m,g): #matrice, dizionario gruppi\n",
    "    centroide=np.zeros(shape=(len(g.keys()),m.shape[1]))\n",
    "    for i in range(len(g.keys())):\n",
    "        centroide[i]=np.sum(m[g[i]], axis=0)/len(g[i])\n",
    "    return(centroide)\n",
    "\n",
    "def kmeans(m,c): #m=matrice stilemi, c=centroidi\n",
    "#con il seguente comando si ottiene un dizionario, dove per chiave c'è l'indice di riga di ciascuna trama e per valori una lista contenente le distanze della trama da ciascuno dei 26 centroidi.\n",
    "    print(\"esecuzione...\")\n",
    "    k=c.shape[0]\n",
    "    distanze=cosine(c,m)\n",
    "#Con il seguente comando a ciascuna lista di distanze viene aggiunto l'indice del centroide che presenta la similarità massima con quella trama \n",
    "    for z in range(len(distanze.keys())):\n",
    "        distanze[z]=[distanze[z],[[i for i, j in enumerate(distanze[z]) if j == max(distanze[z])][0]]]\n",
    "    groups={el:[] for el in range(k)}\n",
    "    centroids=list(range(k))\n",
    "    for i in centroids:\n",
    "        for j in distanze.keys():\n",
    "            if distanze[j][1][0]==centroids[i]: groups[i].append(j)\n",
    "    coesione=k*[0]\n",
    "    for i in range(len(distanze.keys())):\n",
    "        coesione[distanze[i][1][0]]+=distanze[i][0][distanze[i][1][0]]\n",
    "    coesione.append([sum(coesione)]) #mi da la coesione totale\n",
    "    \n",
    "    #nuovi centroidi:\n",
    "    centroids_new=centroidi(m, groups)\n",
    "    distanze_new=cosine(centroids_new,m)\n",
    "    for z in range(len(distanze_new.keys())):\n",
    "        distanze_new[z]=[distanze_new[z],[[i for i, j in enumerate(distanze_new[z]) if j == max(distanze_new[z])][0]]]\n",
    "    groups_new={el:[] for el in range(k)}\n",
    "    centroids=list(range(k))\n",
    "    for i in centroids:\n",
    "        for j in distanze_new.keys():\n",
    "            if distanze_new[j][1][0]==centroids[i]: groups_new[i].append(j) \n",
    "    coesione_new=k*[0]\n",
    "    for i in range(len(distanze_new.keys())):\n",
    "        coesione_new[distanze_new[i][1][0]]+=distanze_new[i][0][distanze_new[i][1][0]]\n",
    "    coesione_new.append([sum(coesione_new)])\n",
    "    \n",
    "    b=1\n",
    "    print(\"interazione numero {}\".format(b))\n",
    "\n",
    "    start_time = time.time()\n",
    "    b=1    \n",
    "    while True:\n",
    "        groups=groups_new\n",
    "        k_centroids=centroids_new\n",
    "        coesione=coesione_new\n",
    "        centroids_new=centroidi(m,groups)\n",
    "        distanze_new=cosine(centroids_new,m)\n",
    "        for z in range(len(distanze_new.keys())):\n",
    "            distanze_new[z]=[distanze_new[z],[[i for i, j in enumerate(distanze_new[z]) if j == max(distanze_new[z])][0]]]\n",
    "    \n",
    "        groups_new={el:[] for el in range(k)}\n",
    "        centroids=list(range(k))\n",
    "        for i in centroids:\n",
    "            for j in distanze_new.keys():\n",
    "                if distanze_new[j][1][0]==centroids[i]: groups_new[i].append(j) \n",
    "    \n",
    "        \n",
    "        coesione_new=k*[0]\n",
    "        for i in range(len(distanze_new.keys())):\n",
    "            coesione_new[distanze_new[i][1][0]]+=distanze_new[i][0][distanze_new[i][1][0]]\n",
    "        coesione_new.append([sum(coesione_new)]) \n",
    "        if coesione_new[-1][0]/coesione[-1][0]<=1:break\n",
    "        else:b=b+1\n",
    "        print(\"interazione numero {}\".format(b))\n",
    "    \n",
    "    print(\"tempo impiegato: %s seconds\" %(time.time() - start_time))\n",
    "    print(\"numero interazioni: {}\".format(b))\n",
    "    return(groups)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263978cb-e6a8-40b7-9b81-b1cb8db57e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_centroids = count_array[[0,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e31fb3-0f8d-4d83-acef-6e7bf26f8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(count_array)\n",
    "gruppi_KM = kmeans(count_array,k_centroids)\n",
    "gruppi_KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220de70-a7ba-4814-8160-c678f8316bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gruppi_KM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c6802-029f-498b-9299-10752d18a9d3",
   "metadata": {},
   "source": [
    "Alex devi cercare di capire a che libri appartengono i di ciascun gruppo (sperando che in ogni caso venga fuori 1,2,3 e 4,5,6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494091b-8f9c-4048-9a81-522227d469b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85645c-68b7-4e13-8d15-6e5eaebd6fcd",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916bf2ad-de90-4f29-8bbf-0ae4a80a7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_cl(m, k): # m = matrix, k = num clusters\n",
    "    p = m.shape[1]\n",
    "    s = m.shape[0]\n",
    "    W = np.empty((s, s))\n",
    "    for i, j in np.ndindex(W.shape):\n",
    "        W[i, j] = cosine(m[i,].reshape(1, -1), m[j,].reshape(1, -1))[0][0]\n",
    "    D = np.diag(np.sum(W, axis = 0))\n",
    "    L = D - W\n",
    "    # D^(1/2) * L * D^(1/2)\n",
    "    L_sym = np.dot(np.dot(np.diag(1/np.sqrt(np.diag(D))), L), np.diag(1/np.sqrt(np.diag(D))))\n",
    "    e_val, e_vec = np.linalg.eig(L_sym)\n",
    "    T = U = e_vec[:, :k]\n",
    "    cl = kmeans(T,T[[1, 2]])\n",
    "    return cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e90184-5a96-4d91-9b1f-84409bf253a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_cl(count_array, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9598c8-5619-4c28-a951-adc7d842418f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb861a-75f8-4d75-b026-dddbcc492fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13390175-a8d6-473c-b398-d54233631e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = count_array.shape[1]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c523192-da31-4e30-898c-aa76466e6ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_array[0,].reshape(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0dd5c2-29c3-4365-bef9-595236a75304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine(count_array[0,].reshape(1, -1), count_array[1,].reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64248f9-3cb5-4c4a-ba29-a9493997bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([\n",
    "#     [1, 2, 3],\n",
    "#     [4, 5, 6]\n",
    "# ])\n",
    "# a\n",
    "# print([i for j in a for i in j])\n",
    "# print([i for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a512c1b-7043-414b-baaf-7446ab415245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in a:\n",
    "#     print(type(i))\n",
    "#     print(i.shape)\n",
    "#     print(np.array(i.reshape(1, -1)))\n",
    "#     print(i.reshape(1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9312e-8263-4794-aea1-c9294f78bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = 3\n",
    "# a = np.empty((s, s))\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336931a4-049d-4a22-9195-067862accbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0\n",
    "# for i, j in np.ndindex(a.shape):\n",
    "#     a[i, j] = k\n",
    "#     k += 1\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4961a-d536-4246-b924-d3894732ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = count_array.shape[0]\n",
    "W = np.empty((s, s))\n",
    "for i, j in np.ndindex(W.shape):\n",
    "    W[i, j] = cosine(count_array[i,].reshape(1, -1), count_array[j,].reshape(1, -1))[0][0]\n",
    "D = np.diag(np.sum(W, axis = 0))\n",
    "L = D - W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a2f79-0fa8-4747-9396-30a23452362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(W, 2))\n",
    "print(np.round(D, 2))\n",
    "print(np.round(L, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ebf96-8c65-4510-b6f0-533ea2fe02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.diag(1/np.sqrt(np.diag(D))), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa596c-c235-40c5-8282-24978c04ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D^(1/2) * L * D^(1/2)\n",
    "L_sym = np.dot(np.dot(np.diag(1/np.sqrt(np.diag(D))), L), np.diag(1/np.sqrt(np.diag(D))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a838536-c99c-4a89-8c52-4c83ef1a49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(L_sym, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c07b3-27c1-4d50-9613-234cc5eb8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(L_sym)\n",
    "\n",
    "# print(\"Eigenvalues:\")\n",
    "# print(np.round(eigenvalues, 2))\n",
    "\n",
    "# print(\"\\nEigenvectors:\")\n",
    "# print(np.round(eigenvectors, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225028f-bee4-4c17-af9b-b88a6fa34b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2 # num cluster\n",
    "T = U = eigenvectors[:, :k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e1217-dad4-423f-ab41-72b432d33e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "T[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6baaa-14d7-446e-9b22-ee5c38adceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gruppi_spectral = kmeans(T,T[[1, 2]])\n",
    "gruppi_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdf7dd-e56f-4e0e-a736-83259c4a08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(U[:,0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbcbdf-b1e1-423c-8a93-bbc0e8d4bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "U[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd94cf-8c62-4610-9c25-a1f9465d625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eigenvectors[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6376e989-8278-47c9-bb4a-eab7ff113a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(np.dot(np.dot(eigenvectors, np.diag(eigenvalues)), eigenvectors.T), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9717c2-1a99-45e3-b209-aa13c68430cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba07d1a-08bf-4249-a3cd-83db208e3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(eigenvalues, np.diag(eigenvalues))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb6ac3-ec5c-4a9f-9296-e6a1e4f1c495",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e8580-fca6-4565-80d3-0cb743b227c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffedeb-e526-44ad-b489-00ec3e9d8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_names = list()\n",
    "for i in range(len(hp_books)): \n",
    "    book_names.append(f\"book_{i+1}\")\n",
    "print(book_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0becd-434b-4451-8af4-692a828346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "for b in book_names:\n",
    "    new_b = \" \".join(books_hp_token[b])\n",
    "    scores[b] = analyzer.polarity_scores(new_b)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47862ed-5111-49d6-b68b-c26755f3e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "pp.pprint(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc84085-6401-40c9-845f-ed1bf2c10d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1][\"neg\"]))\n",
    "for key in sorted_scores:\n",
    "    print(key + \": \")\n",
    "    print(sorted_scores[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8618a7-478c-499b-bf85-d23df5e46955",
   "metadata": {},
   "source": [
    "As we can see the books are perfectly ordered for the negative score, which means that in each book, even though there isn't a worse feeling with year after year, we can find a growing presence of you know who and of the death eaters, sometimes compensated by positive feelings and experiences of the main characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f828d-2ebc-48cf-be72-d9b0d28ba3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_emotions = dict()\n",
    "keys = [\"fear\", \"anger\", \"anticip\", \"trust\", \"surprise\", \"positive\", \"negative\", \"sadness\", \"disgust\", \"joy\", \"anticipation\"]\n",
    "for b in book_names:\n",
    "    print(b)\n",
    "    emotion = dict()\n",
    "    for k in keys:\n",
    "        emotion[k] = 0    \n",
    "    for t in books_hp_token[b]:\n",
    "        e = NRCLex(t).affect_frequencies\n",
    "        # print(\"\\n\\n\", e, \"\\n\\n\")\n",
    "        for k in e.keys():\n",
    "            emotion[k] += e[k]\n",
    "    # for k in keys:\n",
    "        # print(\"len: \", len(books_hp_token[b]))\n",
    "        # print(k, \": \", emotion[k])\n",
    "        # emotion[k] /= len(books_hp_token[b])\n",
    "    book_emotions[b] = emotion\n",
    "    # print()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c67a2-f564-4657-89df-665604726a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_stand = dict()\n",
    "for k1, e in book_emotions.items():\n",
    "    em_stand[k1] = dict()\n",
    "    tot_em = sum(e.values())\n",
    "    for k2 in e.keys():\n",
    "        if k2 != \"anticip\":\n",
    "            em_stand[k1][k2] = e[k2] / tot_em\n",
    "            \n",
    "print(em_stand[\"book_1\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53f513-a9f0-450c-aef1-5b3d500e359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trust, disgust\n",
    "# watch out: negative doesn't give such a good order as with vader\n",
    "sorted_emotions = dict(sorted(em_stand.items(), key=lambda item: item[1][\"anticipation\"]))\n",
    "print(sorted_emotions.keys())\n",
    "# for key in sorted_emotions:\n",
    "#     print(key + \": \")\n",
    "#     print(em_stand[key])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3d54c-07e7-479d-a897-969ce7f980d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em_stand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
