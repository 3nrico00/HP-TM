{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96d9afc-2dd4-4d2a-a627-c5ca9f000074",
   "metadata": {},
   "source": [
    "# <center>HARRY POTTER</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c55f4b-8db6-4323-8130-867912bfe80e",
   "metadata": {},
   "source": [
    "## <center>and the Text Mining Project</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b8bb9-d8f1-4b8f-b503-6d90914c32dd",
   "metadata": {},
   "source": [
    "#### <center>Enrico Carraro, Alex Cecchetto, Virginia Murru</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc09ba7-3352-4005-8b93-233bd1b033b9",
   "metadata": {},
   "source": [
    "1. [Data](#data)<br>\n",
    "2. [Exploratory Data Analysis](#eda)<br>\n",
    "    2.1 [Bi-Grams](#bigrams)<br>\n",
    "3. [Harry Potter and the order of the books](#order)<br>\n",
    "    3.1 [Formulation of the problem](#problem)<br>\n",
    "    3.2 [t-test](#ttest)<br>\n",
    "    3.3 [Harry Potter and the selected words](#words)<br>\n",
    "4. [Similarity between books](#sim)<br>\n",
    "    4.1 [K-means](#km)<br>\n",
    "    4.2 [Spectral Clustering](#spec)<br>  \n",
    "5. [Sentiment analysis](#sent)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a256732-1c14-4251-ac67-d2b0c98ab86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "# from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # Sentiment analysis\n",
    "from nrclex import NRCLex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01430c5f-3d8a-4670-98ae-e2908fccf555",
   "metadata": {},
   "source": [
    "## 1. Data <a id=data> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba02156a-aa5a-428a-ba69-87b2399a82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the books\n",
    "from Functions.processing import read_hp\n",
    "\n",
    "book1 = read_hp('Data/Book1.txt')\n",
    "book2 = read_hp('Data/Book2.txt')\n",
    "book3 = read_hp('Data/Book3.txt')\n",
    "book4 = read_hp('Data/Book4.txt')\n",
    "book5 = read_hp('Data/Book5.txt')\n",
    "book6 = read_hp('Data/Book6.txt')\n",
    "book7 = read_hp('Data/Book7.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35c1fa1-5a66-448f-a281-54e1a5fcd321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the books in a unique object and transform the text to the lower case\n",
    "books = book1 + book2 + book3 + book4 + book5 + book6 + book7\n",
    "books = books.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d4a42-d0d2-4968-ba72-a9f10b22c516",
   "metadata": {},
   "source": [
    "The first thing to do is to preprocess the data:\n",
    "1. create a custom tokenizer in order to obtain a collection of all the words;\n",
    "2. remove the stop_words, using a collection of common words that has no meaning for the analysis;\n",
    "3. reduce the tokens to the stems, that is part of a word, that is common to all of its inflected variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45cbe1a7-a4c6-45c9-ae5e-66efc6f5e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.processing import CustomTokenizer\n",
    "# the costum Tokenizer has been made in order to collect only the words and without considering abbreviation \n",
    "# (like the possessive case) and the punctuation\n",
    "\n",
    "custom_tokenizer = CustomTokenizer()\n",
    "hp_tokens = custom_tokenizer.tokenize(books)\n",
    "hp_tokens = [i[0] for i in hp_tokens if i[1] == 'WORD'] # this is going to collect only the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7690fdd4-8af9-44f5-95c2-2a3d2b451cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stop words from the collection of token\n",
    "with open(\"Data/stop_words.txt\", 'r') as file:\n",
    "    hp_stop_w = [words.strip() for words in file.readlines() if not (words.startswith(\"Page |\") or words.strip() == '')]\n",
    "\n",
    "hp_tokens_sw = [i for i in hp_tokens if i not in hp_stop_w]\n",
    "# remove the following symbols\n",
    "hp_tokens_sw = [word for word in hp_tokens_sw if not word.startswith(\"'\")] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849be29d-2ee2-4bfd-b743-233121d56de7",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis <a id=eda> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccf8fe-6a7a-49fa-97ce-ef0bcb8221a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that counts the frequency of each token and retrieve the most frequent N tokens\n",
    "from Functions.processing import counter\n",
    "hp_counter = counter(hp_tokens_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b208ae-25ef-4481-a6e7-4b41584e5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 45\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# plt.subplot(121)\n",
    "plt.title(\"{} Most frequent words in the Harry Potter series\".format(N))\n",
    "plt.bar(*zip(*hp_counter.most_common(N)), color=\"gold\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3c937-0789-4c98-88e9-07188397f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud of the most frequent tokens\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(hp_tokens_sw))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfe4de-84d8-4cb1-be31-0e8e27bef9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom wordcloud\n",
    "from PIL import Image\n",
    "MASK = np.array(Image.open(\"Images/Sorting_Hat.png\"))\n",
    "MAX_WORDS = 300\n",
    "MAX_FONT_SIZE = 500\n",
    "RELATIVE_SCALING = 0.7\n",
    "\n",
    "hp = WordCloud(\n",
    "    width = 2000, \n",
    "    height = 2000,\n",
    "    mask = MASK,\n",
    "    max_words = MAX_WORDS, \n",
    "    background_color = \"white\",\n",
    "    max_font_size = MAX_FONT_SIZE,\n",
    "    relative_scaling = RELATIVE_SCALING,\n",
    ").generate_from_frequencies(hp_counter.counts)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(hp, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606a94d-7480-41be-9402-70485170c185",
   "metadata": {},
   "source": [
    "### 2.1 BI-GRAMS <a id=bigrams>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrm = nltk.bigrams(hp_tokens_sw)\n",
    "bigram_fd = nltk.FreqDist(bigrm)\n",
    "bigram_fd.most_common(10)\n",
    "\n",
    "bigram_freq = {f'{key[0]} {key[1]}': value for key, value in dict(bigram_fd).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bf9b1-a7f6-4fba-85f7-6262a3672a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "MASK = np.array(Image.open(\"Images/harry.png\"))\n",
    "MAX_WORDS = 500\n",
    "MAX_FONT_SIZE = 500\n",
    "RELATIVE_SCALING = 0.7\n",
    "\n",
    "hp = WordCloud(\n",
    "    width = 1000, \n",
    "    height = 1000,\n",
    "    mask = MASK,\n",
    "    max_words = MAX_WORDS, \n",
    "    background_color = \"white\",\n",
    "    max_font_size = MAX_FONT_SIZE,\n",
    "    relative_scaling = RELATIVE_SCALING,\n",
    ").generate_from_frequencies(bigram_freq)\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(hp, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2155f-19a8-4b6f-9502-a32e943436cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the stems from the collection of books\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "hp_stem = [stemmer.stem(token) for token in hp_tokens_sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dec9d1-f778-4f5c-a2a6-98173ce8bccb",
   "metadata": {},
   "source": [
    "### CREATION OF DICTIONARIES BY BOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f607a1-5295-4804-9f28-885219e89708",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_books = [book1, book2, book3, book4, book5, book6, book7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd01d13-ec40-4a0c-8df7-edb5f2925046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.processing import preproc\n",
    "\n",
    "books_hp_token = {}\n",
    "for i in range(len(hp_books)):\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    books_hp_token[book_name] = preproc(hp_books[i], custom_tokenizer, stop_words = hp_stop_w)\n",
    "\n",
    "books_hp_stem = {}\n",
    "for i in range(len(hp_books)):\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    books_hp_stem[book_name] = [stemmer.stem(token) for token in books_hp_token[book_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00872ed1-1574-4380-b763-cf4245d0a1eb",
   "metadata": {},
   "source": [
    "## 3. Harry Potter and the order of the books <a id=order>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647c0f1-072c-44bb-8937-17c6fca3a438",
   "metadata": {},
   "source": [
    "The goal is to reconstruct the chronological order of the seven books, starting from the analysis of the texts. The idea is to assume that the position of the first and the last book, classificating the books from 2 to 6 only using the words available and thus, analyzing the evolution of the narrative style. \n",
    "\n",
    "To do this, we considered the most frequent words available in both of the starting books, keeping only the ones that are present also in the other 5 books. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "489fb8aa-eff9-49b1-9be6-36659c4aa5db",
   "metadata": {},
   "source": [
    "x = counter(books_hp_stem['book_1']).counts\n",
    "book11 = dict(sorted(x.items(),key=lambda item: item[1], reverse=True))\n",
    "y = counter(books_hp_stem['book_7']).counts\n",
    "book77 = dict(sorted(y.items(),key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86c725-4a36-464a-95fc-64380f417951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the dictionary by the frequence of each stem\n",
    "book11 = counter(books_hp_stem['book_1']).most_common()\n",
    "book77 = counter(books_hp_stem['book_7']).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee41620-65f6-4872-9762-e71a7301670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the most 400 common stems that are in book 1 and book 7\n",
    "book11 = dict(list(book11.items())[:400])\n",
    "book77 = dict(list(book77.items())[:400])\n",
    "\n",
    "# keep only the common strems\n",
    "most_comm_17 = list(set(book11.keys()) & set(book77.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90bac0b-8c69-4916-85bc-0fee049ec29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_comm_tot = most_comm_17.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f74dd-362d-49b1-a664-1c64118530f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the list of the most common stems, keep the one that are also in the other books\n",
    "for i in range(len(books_hp_stem)):\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    most_comm_tot = [i for i in most_comm_tot if i in books_hp_stem[book_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279eb35d-50d2-412b-a919-f2a8ed2b70c3",
   "metadata": {},
   "source": [
    "### 3.1 Formulation of the Problem <a id=problem>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480fe12-d12a-4820-a23f-74f04fe9c6d6",
   "metadata": {},
   "source": [
    "We consider two initial population:\n",
    "1. $W_1$ = *Harry Potter and the Phylosopher stone*\n",
    "2. $W_7$ = *Harry Potter and the deathly hallows*.\n",
    "\n",
    "Let, for *j* = {1, 7}:\n",
    "- $n_{ij}$ be the frequence of the $i$-$th$ stem for $i = 1, \\dots, N$ in book $j$, with $N$ the total number of stems considered;\n",
    "- $N_j = \\sum_{i}n_{ij}$\n",
    "- $\\theta_{ji}$, for $i = 1, \\dots, N$ the probability that the i-th word is in $W_j$.\n",
    "\n",
    "The probability to observe a sample from $W_1$ and $W_7$ follows a multinomial distribution. Using the log likelihood ratio test to compare the two samples we obtain:\n",
    "\n",
    "$$ \\sum_{i=1}^{N} n_i\\log\\frac{\\theta_{7i}}{\\theta_{1i}}$$\n",
    "\n",
    "We can see that every word has a sort of score, like:\n",
    "\n",
    "$$ s_i = \\log{\\frac{\\theta_{7i}}{\\theta_{1i}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480b581-a498-4ad3-845b-c8169ec30961",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict = {key: value for key, value in counter(books_hp_stem['book_1']).counts.items() if key in most_comm_tot}\n",
    "xx = sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True)\n",
    "df_book = pd.DataFrame(xx, columns = ['Stems', 'Count_1'])\n",
    "df_book['Freq_1'] = df_book['Count_1']/df_book['Count_1'].sum()\n",
    "df_book.head()\n",
    "\n",
    "filtered_dict = {key: value for key, value in counter(books_hp_stem['book_7']).counts.items() if key in most_comm_tot}\n",
    "xx = dict(sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True))\n",
    "df_book['Count_7'] = df_book['Stems'].map(xx)\n",
    "df_book['Freq_7'] = df_book['Stems'].map(xx)/df_book['Stems'].map(xx).sum()\n",
    "df_book.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e4989-a8bd-4075-b54c-b94a1158ed7f",
   "metadata": {},
   "source": [
    "The idea is to extend the analysis to the other books, which represent new populations to be classified, and use the total score of each book as a discriminant function. Thus let $W_k$ with $k = 2,3,4,5,6$ be the populations that represent the books from 2 to 6. For these populations, the same results apply as for $W_1$ to $W_7$. So for each $W_k$ with $k = 2,3,4,5,6,7$ we can calculate a measure that represents the comprehensive score assigned to each $W_k$. In particular for each $W_k$ where $N_k = \\sum_i n_{ik}$ one can calculate the average score\n",
    "\n",
    "$$\n",
    "\\bar{s}_k = \\frac{1}{N_k} \\sum_{i=1}^{N} n_{i} \\log \\frac{\\theta_{1i}}{\\theta_{7i}} = \\sum_{i=1}^{N} n_{i}s_i\n",
    "$$\n",
    "\n",
    "To calculate the average scores we need to estimate the parameter vectors $\\theta_1$ and $\\theta_7$. Since both $N_1$ and $N_7$ are high, the probabilities $\\hat{\\theta}_{i1}$ and $\\hat{\\theta}_{i7}$ can be estimated with the corresponding observed frequencies. So we find for all five books to be classified an associated score that represents the positioning of that book. This measure can be interpreted in relative terms to understand which are the furthest books and which are the closest. To make inferences and evaluate the significance of the results obtained, we can also define the variance of $s_k$, for which an unbiased estimate is\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{V}(\\bar{s}_k) = \\frac{1}{N_k(N_k - 1)} \\left( \\sum_{i=1}^{N} n_i s_i^2 - \\frac{1}{N_k} \\left( \\sum_{i=1}^{N} n_i s_i \\right)^2 \\right).\n",
    "$$\n",
    "\n",
    "Given that $N_k$ is large, $s_k - s_k'$ will be approximately normally distributed with variance equal to the sum of the corresponding variances; therefore, to test the significance, we can use the usual $t$-test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442e63b-61c2-4876-9fb1-e18333c99374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.ordering import hp_count\n",
    "\n",
    "# with this loop we add a column for each of the book with their own freq and counts\n",
    "for i in range(2, 7):\n",
    "    book_idx = f\"book_{i}\"\n",
    "    col_count, col_freq = hp_count(book_idx, books_hp_stem, most_comm_tot, df_book)\n",
    "    df_book[f\"Count_{i}\"] = np.array(col_count)\n",
    "    df_book[f\"Freq_{i}\"]= np.array(col_freq)\n",
    "\n",
    "df_book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa62d17-e66d-44bd-8f73-5b111e87b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.ordering import hp_scores\n",
    "\n",
    "scores = {}\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"] = {'score': hp_scores(df_book, i)}\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key=lambda item: item[1]['score'], reverse = True)\n",
    "scores = {k: v for k, v in sorted_scores}\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fd7e2-3c73-44b3-b50f-897f704f36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.ordering import hp_var\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"]['Variance'] = hp_var(df_book, i)\n",
    "    scores[f\"book_{i}\"]['N'] = sum(df_book[f\"Count_{i}\"])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8e20e-b90d-4233-a2e5-126b6573bf41",
   "metadata": {},
   "source": [
    "### 3.2 t-test <a id=ttest>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c252b-cbe7-48f7-836d-81f58b9ac592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "from Functions.ordering import t_test\n",
    "\n",
    "ttest = {}\n",
    "for i in range(2,7):\n",
    "    for j in range(i+1,7):\n",
    "        ttest[f\"test_{i}_{j}\"] = {'t-stat': round(t_test(scores[f\"book_{i}\"], scores[f\"book_{j}\"])[0],3), \n",
    "                              'pval': round(t_test(scores[f\"book_{i}\"], scores[f\"book_{j}\"])[1], 3)}\n",
    "ttest\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9096cf-3c45-4cd3-a580-9304bead610c",
   "metadata": {},
   "source": [
    "The books do not seem to follow a coherent order. Probably, the chosen set of words is very generic, and considering the vastness of the analyzed texts, specific lexical constructs are captured. However, these constructs are not necessarily related to the plot and style of the various books but rather result from the large size of the books.\n",
    "\n",
    "Therefore, an attempt has been made to select words using some form of criteria. In particular, ontological dictionaries were used to identify words that could indicate negative sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee42fa6-9704-4401-98b2-64d54837be21",
   "metadata": {},
   "source": [
    "### 3.3 Harry Potter and the selected words <a id=words>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3775b42-7ac7-4c97-b389-a63814c5b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of NRCLex\n",
    "text_object = NRCLex(book1)\n",
    "\n",
    "# book1\n",
    "emot_1 = text_object.affect_dict\n",
    "set(text_object.affect_list)\n",
    "filter1 = [key for key, value in emot_1.items() if 'anger' in value or 'fear' in value \n",
    "                 or 'negative' in value or 'sadness' in value]\n",
    "\n",
    "# book7\n",
    "text_object = NRCLex(book7)\n",
    "emot_7 = text_object.affect_dict\n",
    "# set(text_object.affect_list)\n",
    "filter7 = [key for key, value in emot_7.items() if 'anger' in value or 'fear' in value \n",
    "                 or 'negative' in value or 'sadness' in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e1fe9-c4c1-406f-9c8b-ea2a23757254",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_comm_filt = list(set(filter1) & set(filter7))\n",
    "# obtain the stems \n",
    "most_comm = [stemmer.stem(token) for token in most_comm_tot]\n",
    "most_comm_tot = most_comm.copy()\n",
    "for i in range(len(books_hp_stem)):\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    most_comm_tot = [i for i in most_comm_tot if i in books_hp_stem[book_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787e73b-0b40-47f4-8b7d-e8f7b4177922",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(most_comm_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27bbb9-3f6b-44ce-82c7-8f2575f74a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_dict = {key: value for key, value in counter(books_hp_stem['book_1']).most_common(300).items() if key in most_comm_tot}\n",
    "filtered_dict = {key: value for key, value in counter(books_hp_stem['book_1']).counts.items() if key in most_comm_tot}\n",
    "xx = sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True)\n",
    "df_book = pd.DataFrame(xx, columns = ['Stems', 'Count_1'])\n",
    "df_book['Freq_1'] = df_book['Count_1']/df_book['Count_1'].sum()\n",
    "# df_book.head()\n",
    "\n",
    "filtered_dict = {key: value for key, value in counter(books_hp_stem['book_7']).counts.items() if key in most_comm_tot}\n",
    "xx = dict(sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True))\n",
    "df_book['Count_7'] = df_book['Stems'].map(xx)\n",
    "df_book['Freq_7'] = df_book['Stems'].map(xx)/df_book['Stems'].map(xx).sum()\n",
    "# df_book.head()\n",
    "\n",
    "for i in range(2, 7):\n",
    "    book_idx = f\"book_{i}\"\n",
    "    col_count, col_freq = hp_count(book_idx, books_hp_stem, most_comm_tot, df_book)\n",
    "    df_book[f\"Count_{i}\"] = np.array(col_count)\n",
    "    df_book[f\"Freq_{i}\"]= np.array(col_freq)\n",
    "\n",
    "# df_book.head()\n",
    "\n",
    "#from func.proc import hp_scores\n",
    "\n",
    "scores = {}\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"] = {'score': hp_scores(df_book, i)}\n",
    "#aggiunta\n",
    "    \n",
    "\n",
    "sorted_scores = sorted(scores.items(), key=lambda item: item[1]['score'], reverse = True)\n",
    "scores = {k: v for k, v in sorted_scores}\n",
    "\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"]['Variance'] = hp_var(df_book, i)\n",
    "    scores[f\"book_{i}\"]['N'] = sum(df_book[f\"Count_{i}\"])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484cb9c-82a5-4087-9cd9-9ecdbd63d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest = {}\n",
    "for i in range(2,7):\n",
    "    for j in range(i+1,7):\n",
    "        ttest[f\"test_{i}_{j}\"] = {'t-stat': round(t_test(scores[f\"book_{i}\"], scores[f\"book_{j}\"])[0],3), \n",
    "                              'pval': round(t_test(scores[f\"book_{i}\"], scores[f\"book_{j}\"])[1], 3)}\n",
    "ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71242910-aef3-4ca4-b058-ca6828c74561",
   "metadata": {},
   "source": [
    "## 4. Similarity between books <a id=sim>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd785d-65a8-4d19-a5b0-81463cd862ca",
   "metadata": {},
   "source": [
    "We start from the matrix of stems, where for each row we have the frequency of each stem within the individual book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb9539-3de9-4e04-b5a8-087dda221187",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\" \".join(words) for words in books_hp_stem.values()]\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "count_array = count_matrix.toarray()\n",
    "\n",
    "freq_matrix = np.divide(count_array, np.sum(count_array, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b99588-f604-4c6e-8477-dfdc32de1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(count_array, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2250c37-4767-4463-834e-1caaf0be0329",
   "metadata": {},
   "source": [
    "### 4.1 k-means <a id=km>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95f64e-eeb0-4fdc-bb75-04033a11aeae",
   "metadata": {},
   "source": [
    "We use a custom kmeans to cluster the seven books. In particular we chose as centroids the first and the last book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263978cb-e6a8-40b7-9b81-b1cb8db57e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.clustering import cosine, centroidi, kmeans\n",
    "\n",
    "k_centroids = count_array[[0,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e31fb3-0f8d-4d83-acef-6e7bf26f8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_array = count_array.copy()\n",
    "np.random.shuffle(sorted_array)\n",
    "gruppi_KM = kmeans(sorted_array, k_centroids)\n",
    "gruppi_KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220de70-a7ba-4814-8160-c678f8316bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#going back to the original order based on the number of stems of each vectr\n",
    "initial_mat = np.sum(count_array, axis = 1)\n",
    "sorted_mat = np.sum(sorted_array, axis = 1)\n",
    "sorted_indices_array1 = np.argsort(initial_mat)\n",
    "positions_in_array1 = sorted_indices_array1[np.argsort(np.argsort(sorted_mat))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840ea5e-d144-4d69-98e2-d437387ffdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for i in gruppi_KM:\n",
    "    clusters[f\"cluster_{i + 1}\"] = [f\"book_{i}\" for i in positions_in_array1[gruppi_KM[i]] + 1]\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85645c-68b7-4e13-8d15-6e5eaebd6fcd",
   "metadata": {},
   "source": [
    "### 4.2 Spectral Clustering <a id=spec>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4de13f-8686-4481-a744-919f305afab8",
   "metadata": {},
   "source": [
    "To perform spectral clustering we first have to represent data as a graph, with vertices and edges, represented in the form $\\mathcal{G} = \\{V, E\\}$. To do so an intuitive way is to use as vertices all the observation (books) and as weights to connect them the distance between each couple of observation. In particular we will use the cosine distance $d(w_i, w_j)$ already defined. In this way we get that all the vertices are connected with weights given by the inputs of matrix W, where\n",
    "$$\n",
    "W = w_{ij}\\quad\\text{with}\\quad w_{ij} = d(v_i, v_j).\n",
    "$$\n",
    "Moreover, we need to define the degree matrix \n",
    "$$\n",
    "D = diag(d_i)\n",
    "$$ \n",
    "with all empty off-diagonal entries, whereas the diagonal contains the degree of each node, which is the number of edges incident on it. We will refer to it as \n",
    "$$\n",
    "d_i = \\sum_{j = 1}^{n}w_{ij}.\n",
    "$$\n",
    "\n",
    "It is now necessary to define the Graph Laplacian $L = D - W$ and normalize it as folows: \n",
    "$$\n",
    "L_{\\text{sym}}:=D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = I - D^{-\\frac{1}{2}}WD^{-\\frac{1}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f4a17-9003-4ef9-9c2a-5aac0dfe4f3f",
   "metadata": {},
   "source": [
    "### Normalized Spectral clustering according to Ng, Jordan, and Weiss (2002)\n",
    "Input: weight matrix $W\\in \\mathbb{R}^{n\\times n}$, number $k$ of clusters to construct\n",
    "* Compute the normalized Laplacian $L_{\\text{sym}}$.\n",
    "* Compute the first $k$ eigenvectors $u_1, \\dots , u_k$ of $L_{\\text{sym}}$ correspondent to the smallest $k$ eigenvalues.\n",
    "* Let $U\\in \\mathbb{R}^{n\\times k}$ be the matrix containing the vectors $u_1, \\dots , u_k$ of $L_{\\text{sym}}$ as columns.\n",
    "* Form the matrix $T\\in \\mathbb{R}^{n\\times k}$ from $U$ by normalizing the rows to norm 1, that is set $t_{ij} = u_{ij}/(\\sum_k u_{ik}^2)^{1/2}$\n",
    "* For $i = 1, \\dots, n$, let $y_i \\in\\mathbb(R){k}$ be the vector corresponding to the $i$-th row of $T$.\n",
    "* Cluster the points $(y_i)_{i = 1, \\dots, n}$ with the $k$-means algorithm into clusters $C_1, \\dots, C_k$.\n",
    "\n",
    "Output: Clusters $A_1, \\dots, A_k$ with $A_i = \\{j\\,|\\,y_j\\in C_i\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916bf2ad-de90-4f29-8bbf-0ae4a80a7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.clustering import spectral_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e90184-5a96-4d91-9b1f-84409bf253a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_cl(count_array, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb6ac3-ec5c-4a9f-9296-e6a1e4f1c495",
   "metadata": {},
   "source": [
    "## 5. Sentiment analysis <a id=sent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e8580-fca6-4565-80d3-0cb743b227c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffedeb-e526-44ad-b489-00ec3e9d8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_names = list()\n",
    "for i in range(len(hp_books)): \n",
    "    book_names.append(f\"book_{i+1}\")\n",
    "print(book_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0becd-434b-4451-8af4-692a828346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for b in book_names:\n",
    "    new_b = \" \".join(books_hp_token[b])\n",
    "    scores[b] = analyzer.polarity_scores(new_b)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b43ec34-2ebf-4ef7-9041-6caeb88394fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc84085-6401-40c9-845f-ed1bf2c10d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = dict(sorted(scores.items(), key = lambda item: item[1][\"neg\"]))\n",
    "\n",
    "for key in sorted_scores:\n",
    "    print(key + \": \")\n",
    "    print(sorted_scores[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261d687-9346-4bde-be5e-abcf2560f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1][\"neu\"]))\n",
    "for key in sorted_scores:\n",
    "    print(key + \": \")\n",
    "    print(sorted_scores[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8618a7-478c-499b-bf85-d23df5e46955",
   "metadata": {},
   "source": [
    "As we can see the books are perfectly ordered for the negative score, which means that in each book, even though there isn't a worse feeling with year after year, we can find a growing presence of you know who and of the death eaters, sometimes compensated by positive feelings and experiences of the main characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f828d-2ebc-48cf-be72-d9b0d28ba3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_emotions = dict()\n",
    "keys = [\"fear\", \"anger\", \"anticip\", \"trust\", \"surprise\", \"positive\", \"negative\", \"sadness\", \"disgust\", \"joy\", \"anticipation\"]\n",
    "for b in book_names:\n",
    "    print(b)\n",
    "    emotion = dict()\n",
    "    for k in keys:\n",
    "        emotion[k] = 0    \n",
    "    for t in books_hp_token[b]:\n",
    "        e = NRCLex(t).affect_frequencies\n",
    "        for k in e.keys():\n",
    "            emotion[k] += e[k]\n",
    "    book_emotions[b] = emotion\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c67a2-f564-4657-89df-665604726a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_stand = dict()\n",
    "for k1, e in book_emotions.items():\n",
    "    em_stand[k1] = dict()\n",
    "    tot_em = sum(e.values())\n",
    "    for k2 in e.keys():\n",
    "        if k2 != \"anticip\":\n",
    "            em_stand[k1][k2] = e[k2] / tot_em\n",
    "            \n",
    "print(em_stand[\"book_1\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53f513-a9f0-450c-aef1-5b3d500e359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trust, disgust\n",
    "# watch out: negative doesn't give such a good order as with vader\n",
    "sorted_emotions = dict(sorted(em_stand.items(), key=lambda item: item[1][\"trust\"]))\n",
    "print(sorted_emotions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3d54c-07e7-479d-a897-969ce7f980d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts=Counter(hp_tokens_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = {}\n",
    "for b, ws in books_hp_token.items():\n",
    "    for w in ws:\n",
    "        a = analyzer.polarity_scores(w)\n",
    "        for k in a:\n",
    "            if  a[k] > 0:\n",
    "                sent[w] = sent.get(w, set())\n",
    "                sent[w].add(k)\n",
    "    \n",
    "\n",
    "\n",
    "for i in sent:\n",
    "    sent[i] = list(sent[i])\n",
    "\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ae2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_book = {}\n",
    "for b in book_names:\n",
    "    sent = {}\n",
    "    for w in books_hp_token[b]:\n",
    "        a = analyzer.polarity_scores(w)\n",
    "        for k in a:\n",
    "            if  a[k] > 0:\n",
    "                sent[w] = sent.get(w, set())\n",
    "                sent[w].add(k)\n",
    "    for i in sent:\n",
    "        sent[i] = list(sent[i])\n",
    "    sent_book[b] = sent\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(sent_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55090dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=Counter(hp_tokens_sw)\n",
    "\n",
    "word_counts=dict()\n",
    "for i in sent.keys():\n",
    "  if (\"pos\" in sent[i]): word_counts[i]=counts[i]\n",
    "\n",
    "\n",
    "surprise=pd.DataFrame.from_dict(word_counts, orient='index')\n",
    "surprise = surprise.reset_index()\n",
    "surprise = surprise.rename(columns={'index' : 'words' , 0: 'count'})\n",
    "surprise=surprise.sort_values(\"count\",ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(surprise['words'][(len(surprise)-15):], surprise['count'][(len(surprise)-15):], color=\"#E5BE01\")\n",
    "#sns.barplot(x=\"words\",y=\"count\", data=anger[50:])\n",
    "plt.title(\"Most common positive words\")\n",
    "plt.xlabel('Frequencies')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts=dict()\n",
    "for i in sent.keys():\n",
    "  if (\"neg\" in sent[i]): word_counts[i]=counts[i]\n",
    "\n",
    "\n",
    "surprise=pd.DataFrame.from_dict(word_counts, orient='index')\n",
    "surprise = surprise.reset_index()\n",
    "surprise = surprise.rename(columns={'index' : 'words' , 0: 'count'})\n",
    "surprise=surprise.sort_values(\"count\",ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(surprise['words'][(len(surprise)-15):], surprise['count'][(len(surprise)-15):], color=\"#003399\")\n",
    "#sns.barplot(x=\"words\",y=\"count\", data=anger[50:])\n",
    "plt.title(\"Most common negative words\")\n",
    "plt.xlabel('Frequencies')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
