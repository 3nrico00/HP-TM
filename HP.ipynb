{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96d9afc-2dd4-4d2a-a627-c5ca9f000074",
   "metadata": {},
   "source": [
    "# HARRY POTTER TEXT MINING DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a256732-1c14-4251-ac67-d2b0c98ab86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from string import punctuation\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # Sentiment analysis\n",
    "import pprint # to print dictionaries\n",
    "from nrclex import NRCLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02156a-aa5a-428a-ba69-87b2399a82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.proc import read_hp\n",
    "\n",
    "book1 = read_hp('Book1.txt')\n",
    "book2 = read_hp('Book2.txt')\n",
    "book3 = read_hp('Book3.txt')\n",
    "book4 = read_hp('Book4.txt')\n",
    "book5 = read_hp('Book5.txt')\n",
    "book6 = read_hp('Book6.txt')\n",
    "book7 = read_hp('Book7.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3dd69-fbe5-4a7c-b112-164573c9b4d3",
   "metadata": {},
   "source": [
    "I collect all the books together and transform to the lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c1fa1-5a66-448f-a281-54e1a5fcd321",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = book1 + book2 + book3 + book4 + book5 + book6 + book7\n",
    "books = books.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d4a42-d0d2-4968-ba72-a9f10b22c516",
   "metadata": {},
   "source": [
    "I start managing my text:\n",
    "1. create my own tokenizer:\n",
    "    1. consider all the words\n",
    "    2. consider common "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72234f3c-c343-4edd-9110-22215fc4e06f",
   "metadata": {},
   "source": [
    "from this list I want to keep only the words without considering the punctuation and the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbe1a7-a4c6-45c9-ae5e-66efc6f5e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.proc import CustomTokenizer\n",
    "\n",
    "custom_tokenizer = CustomTokenizer()\n",
    "hp_tokens = custom_tokenizer.tokenize(books)\n",
    "hp_tokens = [i[0] for i in hp_tokens if i[1] == 'WORD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d031b-36f5-4130-8bc2-9b146ab590b1",
   "metadata": {},
   "source": [
    "now that i have the tokens from the books, i'm going to delete the stop words (taken from a common dict and defined by us using some frequences statics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690fdd4-8af9-44f5-95c2-2a3d2b451cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words.txt = stopwords tidytext from R\n",
    "with open(\"stop_words.txt\", 'r') as file:\n",
    "    hp_stop_w = [words.strip() for words in file.readlines() if not (words.startswith(\"Page |\") or words.strip() == '')]\n",
    "\n",
    "hp_tokens_sw = [i for i in hp_tokens if i not in hp_stop_w]\n",
    "hp_tokens_sw = [word for word in hp_tokens_sw if not word.startswith(\"'\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b208ae-25ef-4481-a6e7-4b41584e5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_counter = Counter(hp_tokens_sw)\n",
    "\n",
    "N = 45\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# plt.subplot(121)\n",
    "plt.title(\"{} Most frequent words in the Harry Potter series\".format(N))\n",
    "plt.bar(*zip(*hp_counter.most_common(N)), color=\"gold\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3c937-0789-4c98-88e9-07188397f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(hp_tokens_sw))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfe4de-84d8-4cb1-be31-0e8e27bef9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "MASK = np.array(Image.open(\"Sorting_Hat.png\"))\n",
    "MAX_WORDS = 200\n",
    "MAX_FONT_SIZE = 500\n",
    "RELATIVE_SCALING = 0.7\n",
    "\n",
    "hp = WordCloud(\n",
    "    width=500, \n",
    "    height=300,\n",
    "    mask = MASK,\n",
    "    max_words = MAX_WORDS, \n",
    "    background_color = \"white\",\n",
    "    max_font_size = MAX_FONT_SIZE,\n",
    "    relative_scaling = RELATIVE_SCALING,\n",
    ").generate_from_frequencies(hp_counter)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(hp, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2155f-19a8-4b6f-9502-a32e943436cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "hp_stem = [stemmer.stem(token) for token in hp_tokens_sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dec9d1-f778-4f5c-a2a6-98173ce8bccb",
   "metadata": {},
   "source": [
    "## ORA LO FACCIAMO PER LIBRO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f607a1-5295-4804-9f28-885219e89708",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_books = [book1, book2, book3, book4, book5, book6, book7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd01d13-ec40-4a0c-8df7-edb5f2925046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.proc import preproc\n",
    "\n",
    "# Assuming you are in a loop\n",
    "books_hp_token = {}  # Dictionary to store the lists\n",
    "\n",
    "for i in range(len(hp_books)):\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    books_hp_token[book_name] = preproc(hp_books[i], custom_tokenizer, stop_words = hp_stop_w)\n",
    "\n",
    "books_hp_stem = {}\n",
    "for i in range(len(hp_books)):\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    books_hp_stem[book_name] = [stemmer.stem(token) for token in books_hp_token[book_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00872ed1-1574-4380-b763-cf4245d0a1eb",
   "metadata": {},
   "source": [
    "## PLATONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875728c1-1cde-448b-a04c-435d9b57c66b",
   "metadata": {},
   "source": [
    "### SPIEGONE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca0f86-c8b7-4962-9986-257f8ff43084",
   "metadata": {},
   "source": [
    "Inizialmente partiamo prendendo le 400 parole più frequenti all'interno del primo e del settimo libro (che saranno gli estremi del nostro ordinamento di libri) e di queste parole, terremo quelle che sono presenti anche negli altri 5 libri eliminando le parole che ai fini dell'analisi non ci sembrano rilevanti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90df9c-55d4-4c46-9298-72bab59c1c76",
   "metadata": {},
   "source": [
    "Dentro `books_hp_stem` abbiamo gli stemmi per ciascun libro, ora ci creiamo un dizionario che conti le parole del primo e del settimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fe20a-eff7-42f2-88f1-059608942a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Counter(books_hp_stem['book_1'])\n",
    "book11 = dict(sorted(x.items(),key=lambda item: item[1], reverse=True))\n",
    "y = Counter(books_hp_stem['book_7'])\n",
    "book77 = dict(sorted(y.items(),key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a698a-77c3-444b-9ee1-e1a28321090d",
   "metadata": {},
   "source": [
    "Ora quindi ci teniamo le prime 400 parole dal primo e dal settimo libro che sono quelli che mi definiranno l'ordinamento dei libri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee41620-65f6-4872-9762-e71a7301670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the most common stems that are in book 1 and book 7\n",
    "book11 = dict(list(book11.items())[:400])\n",
    "book77 = dict(list(book77.items())[:400])\n",
    "most_comm_17 = list(set(book11.keys()) & set(book77.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3d7cf-003c-4a61-a63d-24d515e84d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_comm_tot = most_comm_17.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f74dd-362d-49b1-a664-1c64118530f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the most common stems that are also in the other books\n",
    "for i in range(len(books_hp_stem)):  # Replace 5 with the desired number of iterations\n",
    "    book_name = f\"book_{i+1}\"\n",
    "    most_comm_tot = [i for i in most_comm_tot if i in books_hp_stem[book_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480fe12-d12a-4820-a23f-74f04fe9c6d6",
   "metadata": {},
   "source": [
    "We consider two initial population $W_1$ = *Harry Potter and the Phylosopher stone* and $W_7$ = *Harry Potter and the deathly hallows*. Let $n_{i1}$ the frequence of the *i*-th stem for *i* = 1, $\\dots$, $N_{stems}$ in book 1 and let $N_1 = \\sum_{i}n_{i1}$ and $N_7$ the corrispondent sum for $W_7$. We indicate with $\\theta_{1i}$, for $i = 1, \\dots, p$ the probability that the i-th word is in $W_1$ and $\\theta_{7i}$ for $i = 1, \\dots, p$ the same quantity for $W_7$. The probability to observe a sample from $W_1$ and $W_7$ follows a multinomial distribution. Using the log likelihood ratio test to compare the two samples we get the expression:\n",
    "\n",
    "$$ \\sum_{i=1}^{N_{stem}} n_i\\log\\frac{\\theta_{7i}}{\\theta_{1i}}$$\n",
    "\n",
    "We can see that every word has a sort of score, like:\n",
    "\n",
    "$$ s_i = \\log{\\frac{\\theta_{7i}}{\\theta_{1i}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480b581-a498-4ad3-845b-c8169ec30961",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict = {key: value for key, value in Counter(books_hp_stem['book_1']).items() if key in most_comm_tot}\n",
    "xx = sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True)\n",
    "df_book = pd.DataFrame(xx, columns = ['Stems', 'Count_1'])\n",
    "df_book['Freq_1'] = df_book['Count_1']/df_book['Count_1'].sum()\n",
    "df_book.head()\n",
    "\n",
    "filtered_dict = {key: value for key, value in Counter(books_hp_stem['book_7']).items() if key in most_comm_tot}\n",
    "xx = dict(sorted(filtered_dict.items(),key=lambda item: item[1], reverse=True))\n",
    "df_book['Count_7'] = df_book['Stems'].map(xx)\n",
    "df_book['Freq_7'] = df_book['Stems'].map(xx)/df_book['Stems'].map(xx).sum()\n",
    "df_book.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e4989-a8bd-4075-b54c-b94a1158ed7f",
   "metadata": {},
   "source": [
    "Now the idea is to extend the analysis to the other books, which represent new populations to be classified, and use the total score of each book as a discriminant function. Thus let $W_k$ with $k = 2,3,4,5,6$ be the populations that represent the books from 2 to 6. For these populations, the same results apply as for $W_1$ to $W_7$. So for each $W_k$ with $k = 2,3,4,5,6,7$ we can calculate a measure that represents the comprehensive score assigned to each $W_k$. In particular for each $W_k$ where $N_k = \\sum_i n_{ik}$ one can calculate the average score\n",
    "\n",
    "$$\n",
    "\\bar{s}_k = \\frac{1}{N_k} \\sum_{i=1}^{N_{stem}} n_{i} \\log \\frac{\\theta_{1i}}{\\theta_{7i}} = \\sum_{i=1}^{N_{stem}} n_{i}s_i \\quad i = 1, ..., N_{stem}.\n",
    "$$\n",
    "\n",
    "To calculate the average scores we need to estimate the parameter vectors $\\theta_1$ and $\\theta_7$. Since both $N_1$ and $N_7$ are high, the probabilities $\\hat{\\theta}_{i1}$ and $\\hat{\\theta}_{i7}$ can be estimated with the corresponding observed frequencies. So we find for all five books to be classified an associated score that represents the positioning of that book. This measure can be interpreted in relative terms to understand which are the furthest books and which are the closest. To make inferences and evaluate the significance of the results obtained, we can also define the variance of $s_k$, for which an unbiased estimate is\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{V}(\\bar{s}_k) = \\frac{1}{N_k(N_k - 1)} \\left( \\sum_{i=1}^{N_{stem}} n_i s_i^2 - \\frac{1}{N_k} \\left( \\sum_{i=1}^{N_{stem}} n_i s_i \\right)^2 \\right).\n",
    "$$\n",
    "\n",
    "Given that $N_k$ is large, $s_k - s_k'$ will be approximately normally distributed with variance equal to the sum of the corresponding variances; therefore, to test the significance, we can use the usual $t$-test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442e63b-61c2-4876-9fb1-e18333c99374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.proc import hp_count\n",
    "\n",
    "# with this loop we add a column for each of the book with their own freq and counts\n",
    "for i in range(2, 7):\n",
    "    book_idx = f\"book_{i}\"\n",
    "    col_count, col_freq = hp_count(book_idx, books_hp_stem, most_comm_tot, df_book)\n",
    "    df_book[f\"Count_{i}\"] = np.array(col_count)\n",
    "    df_book[f\"Freq_{i}\"]= np.array(col_freq)\n",
    "\n",
    "df_book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa62d17-e66d-44bd-8f73-5b111e87b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.proc import hp_scores\n",
    "\n",
    "scores = {}\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"] = {'score': hp_scores(df_book, i)}\n",
    "#aggiunta\n",
    "    \n",
    "\n",
    "sorted_scores = sorted(scores.items(), key=lambda item: item[1]['score'], reverse = True)\n",
    "scores = {k: v for k, v in sorted_scores}\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fd7e2-3c73-44b3-b50f-897f704f36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.proc import hp_var\n",
    "for i in range(2, 7):\n",
    "    scores[f\"book_{i}\"]['Variance'] = hp_var(df_book, i)\n",
    "    scores[f\"book_{i}\"]['N'] = sum(df_book[f\"Count_{i}\"])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8e20e-b90d-4233-a2e5-126b6573bf41",
   "metadata": {},
   "source": [
    "### t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c252b-cbe7-48f7-836d-81f58b9ac592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from func.proc import t_test\n",
    "\n",
    "t_statistic, p_value = t_test(scores[\"book_2\"], scores[\"book_6\"])\n",
    "\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800b58d-bcd0-4954-92a2-1a0b6ded889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest = {}\n",
    "for i in range(2,7):\n",
    "    for j in range(i+1,7):\n",
    "        ttest[f\"test_{i}_{j}\"] = {'t-stat': round(t_test(scores[f\"book_{i}\"], scores[f\"book_{j}\"])[0],3), \n",
    "                              'pval': round(t_test(scores[f\"book_{i}\"], scores[f\"book_{j}\"])[1], 3)}\n",
    "ttest\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9096cf-3c45-4cd3-a580-9304bead610c",
   "metadata": {},
   "source": [
    "Abbiamo detto che ci sta che i libri siano ordinati un po' a cazzum, infatti questo ordinamento è stato fatto utilizzando parole senza nessun criterio. Probabilmente l’insieme di parole scelto e molto generico e, considerando anche la vastit`a dei testi analizzati, vengono colti specifici costrutti lessicali, che non sono pero' legati alla trama e allo stile dei vari libri, ma sono il risultato dell’elevata dimensione dei libri.\n",
    "\n",
    "Inoltre il numero di parole scelto per l’analisi potrebbe essere troppo esiguo in relazione alla dimensione del problema.\n",
    "Abbiamo quindi replicato l’analisi dando una direzione piu specifica al problema, cioè assumendo l’ipotesi di un’evoluzione dello stile narrativo dell’autrice che ci ha portato a svolgere questo progetto. Partendo da diversi elenchi di parole categorizzate da dizionari ontologici come indicative di sentimenti negativi (come paura, tristezza, violenza) abbiamo selezionato manualmente all’interno di W1 e W7 le piu` frequenti, anche in relazione al contesto della saga. Dopo aver tenuto solamente le parole presenti in tutti e sette i libri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71242910-aef3-4ca4-b058-ca6828c74561",
   "metadata": {},
   "source": [
    "## SIMILARITY "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd785d-65a8-4d19-a5b0-81463cd862ca",
   "metadata": {},
   "source": [
    "We start from the stemmed words matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb9539-3de9-4e04-b5a8-087dda221187",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\" \".join(words) for words in books_hp_stem.values()]\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "count_array = count_matrix.toarray()\n",
    "\n",
    "freq_matrix = np.divide(count_array, np.sum(count_array, axis=1, keepdims=True))\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196547e3-8cd4-4c3f-83ef-7c887af6dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\" \".join(words) for words in books_hp_stem.values()]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(documents)\n",
    "count_array = count_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b47b4-35a2-47da-9a69-5b8cf2b8dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b99588-f604-4c6e-8477-dfdc32de1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(count_array, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0ab9e-c5a7-4aa0-9548-6563b10abc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(books_hp_stem['book_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263978cb-e6a8-40b7-9b81-b1cb8db57e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.proc import cosine, centroidi, kmeans\n",
    "\n",
    "k_centroids = count_array[[0,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e31fb3-0f8d-4d83-acef-6e7bf26f8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(count_array)\n",
    "gruppi_KM = kmeans(count_array,k_centroids)\n",
    "gruppi_KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220de70-a7ba-4814-8160-c678f8316bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gruppi_KM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c6802-029f-498b-9299-10752d18a9d3",
   "metadata": {},
   "source": [
    "Alex devi cercare di capire a che libri appartengono i di ciascun gruppo (sperando che in ogni caso venga fuori 1,2,3 e 4,5,6,7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85645c-68b7-4e13-8d15-6e5eaebd6fcd",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4de13f-8686-4481-a744-919f305afab8",
   "metadata": {},
   "source": [
    "To perform spectral clustering we first have to represent data as a graph, with vertices and edges, represented in the form $\\mathcal{G} = \\{V, E\\}$. To do so an intuitive way is to use as vertices all the observation (books) and as weights to connect them the distance between each couple of observation. In particular we will use the cosine distance $d(w_i, w_j)$ already defined. In this way we get that all the vertices are connected with weights given by the inputs of matrix W, where\n",
    "$$\n",
    "W = w_{ij}\\quad\\text{with}\\quad w_{ij} = d(v_i, v_j).\n",
    "$$\n",
    "Moreover, we need to define the degree matrix \n",
    "$$\n",
    "D = diag(d_i)\n",
    "$$ \n",
    "with all empty off-diagonal entries, whereas the diagonal contains the degree of each node, which is the number of edges incident on it. We will refer to it as \n",
    "$$\n",
    "d_i = \\sum_{j = 1}^{n}w_{ij}.\n",
    "$$\n",
    "\n",
    "It is now necessary to define the Graph Laplacian $L = D - W$ and normalize it as folows: \n",
    "$$\n",
    "L_{\\text{sym}}:=D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = I - D^{-\\frac{1}{2}}WD^{-\\frac{1}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f4a17-9003-4ef9-9c2a-5aac0dfe4f3f",
   "metadata": {},
   "source": [
    "### Normalized Spectral clustering according to Ng, Jordan, and Weiss (2002)\n",
    "Input: weight matrix $W\\in \\mathbb{R}^{n\\times n}$, number $k$ of clusters to construct\n",
    "* Compute the normalized Laplacian $L_{\\text{sym}}$.\n",
    "* Compute the first $k$ eigenvectors $u_1, \\dots , u_k$ of $L_{\\text{sym}}$ correspondent to the smallest $k$ eigenvalues.\n",
    "* Let $U\\in \\mathbb{R}^{n\\times k}$ be the matrix containing the vectors $u_1, \\dots , u_k$ of $L_{\\text{sym}}$ as columns.\n",
    "* Form the matrix $T\\in \\mathbb{R}^{n\\times k}$ from $U$ by normalizing the rows to norm 1, that is set $t_{ij} = u_{ij}/(\\sum_k u_{ik}^2)^{1/2}$\n",
    "* For $i = 1, \\dots, n$, let $y_i \\in\\mathbb(R){k}$ be the vector corresponding to the $i$-th row of $T$.\n",
    "* Cluster the points $(y_i)_{i = 1, \\dots, n}$ with the $k$-means algorithm into clusters $C_1, \\dots, C_k$.\n",
    "\n",
    "Output: Clusters $A_1, \\dots, A_k$ with $A_i = \\{j\\,|\\,y_j\\in C_i\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916bf2ad-de90-4f29-8bbf-0ae4a80a7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.proc import spectral_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e90184-5a96-4d91-9b1f-84409bf253a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_cl(count_array, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb6ac3-ec5c-4a9f-9296-e6a1e4f1c495",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e8580-fca6-4565-80d3-0cb743b227c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffedeb-e526-44ad-b489-00ec3e9d8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_names = list()\n",
    "for i in range(len(hp_books)): \n",
    "    book_names.append(f\"book_{i+1}\")\n",
    "print(book_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0becd-434b-4451-8af4-692a828346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "for b in book_names:\n",
    "    new_b = \" \".join(books_hp_token[b])\n",
    "    scores[b] = analyzer.polarity_scores(new_b)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc84085-6401-40c9-845f-ed1bf2c10d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1][\"neg\"]))\n",
    "for key in sorted_scores:\n",
    "    print(key + \": \")\n",
    "    print(sorted_scores[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261d687-9346-4bde-be5e-abcf2560f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1][\"neu\"]))\n",
    "for key in sorted_scores:\n",
    "    print(key + \": \")\n",
    "    print(sorted_scores[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8618a7-478c-499b-bf85-d23df5e46955",
   "metadata": {},
   "source": [
    "As we can see the books are perfectly ordered for the negative score, which means that in each book, even though there isn't a worse feeling with year after year, we can find a growing presence of you know who and of the death eaters, sometimes compensated by positive feelings and experiences of the main characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f828d-2ebc-48cf-be72-d9b0d28ba3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_emotions = dict()\n",
    "keys = [\"fear\", \"anger\", \"anticip\", \"trust\", \"surprise\", \"positive\", \"negative\", \"sadness\", \"disgust\", \"joy\", \"anticipation\"]\n",
    "for b in book_names:\n",
    "    print(b)\n",
    "    emotion = dict()\n",
    "    for k in keys:\n",
    "        emotion[k] = 0    \n",
    "    for t in books_hp_token[b]:\n",
    "        e = NRCLex(t).affect_frequencies\n",
    "        for k in e.keys():\n",
    "            emotion[k] += e[k]\n",
    "    book_emotions[b] = emotion\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c67a2-f564-4657-89df-665604726a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_stand = dict()\n",
    "for k1, e in book_emotions.items():\n",
    "    em_stand[k1] = dict()\n",
    "    tot_em = sum(e.values())\n",
    "    for k2 in e.keys():\n",
    "        if k2 != \"anticip\":\n",
    "            em_stand[k1][k2] = e[k2] / tot_em\n",
    "            \n",
    "print(em_stand[\"book_1\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53f513-a9f0-450c-aef1-5b3d500e359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trust, disgust\n",
    "# watch out: negative doesn't give such a good order as with vader\n",
    "sorted_emotions = dict(sorted(em_stand.items(), key=lambda item: item[1][\"trust\"]))\n",
    "print(sorted_emotions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3d54c-07e7-479d-a897-969ce7f980d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(em_stand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
